\documentclass[10pt]{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[english]{babel} % hyphenation
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\include{meta.tex}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Functional magnetic resonance imaging (fMRI) data has considerable potential
  for predicting neuropsychological and neurophysiological disorders. However, processing this data
  for use in machine learning (ML) and/or deep learning (DL) algorithms is challenging. In this
  paper we implement both novel deep learning architectures and a preprocessing step for converting
  fMRI images into spatially-rich 4D summary images with the same or greater predictive potential as
  the raw fMRI. We show that tuned models on these summary "eigen-perturbation" images are just as
  accurate as on the fMRI original images, despite a \(10-20\) times reduction in size, and achieve
  state-of-the-art accuracy of [over 70\% we hope!] on a \emph{large}, cross-site validation set.
\end{abstract}
\keywords{fMRI \and ABIDE \and deep learing \and convolutional neural network \and random matrix theory \and eigenvalues \and perturbation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Key Articles}

\subsection{\citet{heinsfeldIdentificationAutismSpectrum2018}}

\begin{itemize}
  \item 70\% with C-PAC preprocessing pipeline
  \item used CC200 atlas to reduce to ROI feature vectors (i.e. not CNN)
  \item actually used functional connectivity as the predictor
  \item used 10-fold validation across all sites (good!)
  \item used a deep denoising autoencoder (really just MLP with some augmentation / dropout on inputs)
\end{itemize}

\subsection{\citet{liMultisiteFMRIAnalysis2020}}

\begin{itemize}
  \item 70\% with C-PAC preprocessing pipeline
  \item similar number of subjects to us
  \item used sliding windows (32 timepoints long) of mean ROI sequences (HO Atlas, 111 ROIs)
  \item interesting comparison points is \emph{sex classification accuracy} which was only between
  \item report no overall accuracy, but NYU hardest to classify (67.6\% at best), USM easiest (up to
  84.9\%)
  \item validation splits are unclear
\end{itemize}

\subsection{\citet{el-gazzarHybrid3DCNN3DCLSTM2019}}

\begin{itemize}
  \item used Conv3D to Conv1D or ConvLSTM (they test both)
  \item CPAC pipeline for ABIDE
  \item only use single-site validation (losers) also NYU and UM (easiest)
  \item also do one multi-site with 19 sites 1100 subjects
  \item patch-based training where prediction is average prediction over crops
  \item max 5-fold acc of \(0.77 \pm 0.05\) on NYU with Conv3dConvLSTM3d
\end{itemize}

\begin{table}
\caption{\citet{el-gazzarHybrid3DCNN3DCLSTM2019} 5-fold cross-site results}
\centering
  \begin{tabular}{lll}
    \toprule
    % \multicolumn{2}{c}{Part}                   \\
    % \cmidrule(r){1-2}
    Model     & Accuracy     & F1-score \\
    \midrule
    AE MLP        [8]   &  0.63 \(\pm\) 0.02  & 0.64 \\
    SVM           [5]   &  0.58 \(\pm\) 0.04  & 0.60 \\
    1D Conv       [7]   &  0.64 \(\pm\) 0.06  & 0.64 \\
    3DCNN 1D     (ours) &  0.54 \(\pm\) 0.02  & 0.50 \\
    3DCNN C-LSTM (ours) &  0.58 \(\pm\) 0.03  & 0.53 \\

    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\section{Headings: first level}
\label{sec:headings}

See Section \ref{sec:headings}.

\subsection{Headings: second level}
\paragraph{Paragraph}

A footnote \footnote{Sample of the first footnote.}.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}