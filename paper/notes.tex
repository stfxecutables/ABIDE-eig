\documentclass[10pt]{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[english]{babel} % hyphenation
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}        % https://tex.stackexchange.com/questions/41035/what-is-causing-undefined-control-sequence
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{bm}             % proper bolding
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\newcommand{\tsub}[2]{\mathbf{#1}_{\text{#2}}}
\DeclareMathOperator*{\argmin}{\arg\min}

\include{meta}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Resting-state functional magnetic resonance imaging (rs-fMRI) data has considerable potential for
  predicting neuropsychological and neurophysiological disorders, especially with modern machine
  learning (ML) and deep learning (DL) techniques. However, the low signal-to-noise ratio, and high
  dimensionality of this data make preparing rs-fMRI data for use in (ML) and/or (DL) challenging.
  We develop a preprocessing step for converting rs-fMRI images into spatially-rich 4D summary
  images by examining the eigenvalues of perturbations to the functional connectivity (FC). We use
  both classical ML algorithms and novel DL architectures developed to exploit this 4D information,
  and show that these "eigenperturbation" images have equal or greater predictive potential to the
  raw fMRI or other common FC-based approaches. We demonstrate the potential utility of these
  eigenperturbation images using the difficult ABIDE dataset, and obtain start of the art overall
  accuracies of [70\%-80\%].
\end{abstract}
\keywords{fMRI \and ABIDE \and deep learing \and convolutional neural network \and random matrix theory \and eigenvalues \and perturbation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Despite tremendous successes in computer vision and natural language processing, machine learning
(ML) and deep learning (DL) algorithms capable of handling higher-dimensional natural data are much
less well-established. The handling of 3D medical images is particularly challenging, due to
necessary pre-processing, unique normalization challenges, larger compute costs, and limited
sample sizes \citep[see][for a general review]{singh3DDeepLearning2020}.

\footnotesize
\textbf{[insert tedious paragraph introducing fMRI here]}
\normalsize

Functional magnetic resonance images (fMRI) are especially challenging. The 4D nature of fMRI
means that a single preprocessed fMRI image can be hundreds of megabytes. A single fMRI is
essentially a (3D) video file\footnote{Except that while in a video file pixel values are in \([0,
255]\), in fMRI the voxel values can be any floating point value.}, and while there are well-known
and well-tested networks that are freely-available pre-trained for processing 2D \emph{images} (e.g.
ResNet, ResNext, MobileNet), efficient and effective models for processing even 2D video data are
still just beginning to be developed
\citep{xieRethinkingSpatiotemporalFeature2018,tranCloserLookSpatiotemporal2018,wangVideoModelingCorrelation2020}.
Attempts to process fMRI data in full are even less developed [cite examples].

In addition, there is only very limited publicly availabile fMRI data. While databases such as
OpenNeuro \citep{markiewiczOpenNeuroOpenResource2021} exist and cover a wide variety of domains, the
number of subjects and is typically far too small for machine learning applications, and the studies
are of very  low quality, and far too heterogenous to be usefully combined.

Perhaps the most popular publicly available fMRI dataset is the ABIDE I dataset, which includes 1112
resting-state fMRI (rs-fMRI) scans from 539 subjects with autism spectrum disorders (ASD) and 573
age-matched controls with typical development (TD) \citep{dimartinoAutismBrainImaging2014}. However,
while the ABIDE I dataset is an appropriate size for ML and DL approaches, it is particularly
challenging to work with due to the inclusion of subjects from 20 different sites. The resulting
varied scanning parameters, demographics, and psychological characteristics means that state-of-the
art classification accuracies are only on the order of 70-75\% (e.g.
\hyperref[tab:existing-attempts]{Table \ref{tab:existing-attempts}}).

Additionally, the ADHD-200 data \citep{bellecNeuroBureauADHD2002017} is a pre-processed rs-fMRI
dataset of 585 TD and 362 children with Attention Deficit Hyperactive Disorder (ADHD), also
containing data from 8 sites and 17 different studies. As with ABIDE, accurate classification is
highly challenging, and competition accuracies originally ranged only from 37.4--60.5\%
\citep{milhamAdhd200ConsortiumModel2012}, with more recent results in the range of 69-76\%
\citep[see][for a brief review]{liuDeepSpatioTemporalRepresentation2021}.

Thus, there is a need for \emph{general} methods to extract useful features from rs-fMRI. Being able
to handle such heterogenous and varied data is also essential for automated prediction procedures that
are to be useful in clinical settings, or for any research that seeks to acquire truly generalizable
insights from fMRI data.

\subsection{Prediction from 4D Spatial or Spatiotemporal Data}

% \subsubsection{Prediction-Focused Analysis}
% When explanation and prediction are in conflict, such as when complex models significantly and
% consistently outperform simpler, more comprehensible models, prediction is necessarily the more
% important practical goal and fact (i.e. explanation must strive to catch up with the predictive
% performance). For sufficiently complex phenomena (quantum phenomena, chaotic systems, human
% cognition) explanation generally may simply not be a realistic or practically useful goal. Thus, we
% focus on predictive analysis.

\subsubsection{Deep Learning Approaches}

As should be clear from \hyperref[tab:existing-attempts]{Table \ref{tab:existing-attempts}}, all
state-of-the-art classification accuracies are obtained either using a pure DL approach or a feature
selection plus DL approach. While other hybrid approaches will likely be developed in the future and
achieve even more impressive results, it is likely that any approach will incorporate \emph{some} DL
methods.

In deep learning, it is common to make a distinction between spatial and channel dimensions. For
example, colour images, with three colour dimensions (red, green, and blue) and two spatial
dimensions (height, width) are usually described as multi-channel 2D. Color video is thus
multi-channel 3D: there are three color channels, and 3 spatial dimensions (height, width, and
time). Likewise, MRI is one-channel 3D, diffusion MRI is multi-channel 3D, and rs-fMRI is
one-channel 4D.

Channel dimensions are special in that their \emph{ordering} is largely arbitrary: shuffling
the order of the channels has no meaningful impact on the semantics or usable information in the
inputs: channels are parallel (but not necessarily independent) features with identical spatial
dimensions.

Given the success of the 2D convolutional neural network (CNN) in computer vision, the most natural
approach to processing video is thus to take successful multi-channel 2D networks (e.g. ResNet) and
expand the convolutions to be 3D. However, the computational costs of 3D convolution are much
larger. While 2D convolution has a computational complexity of \(\mathcal{O}(CHW)\) for channels
\(C\), height \(H\) and width \(W\), 3D convolution has complexity \(\mathcal{O}(CHWD)\) for depth
\(D\) \citep{tranVideoClassificationChannelSeparated2019}. 2D CNNs typically crop or downsample
images to e.g. \(N_p = 256\times256 = 2^9\) pixels and have 3 channels in the input layers, and then
internal layers have from \(2^8\) up to \(2^{10}\) or more channels, with anywhere from tens to
hundreds of such layers, i.e. the complexity is in practice more like over \(\mathcal{O}(2^{17} * L)\)
for a network with number of layers \(L\). By contrast, even a low-resolution MRI scan with
dimensions of \(64 \times 64 \times 64\) is already \(2^{18}\) voxels, and for fMRI, which may have, say,
120 or more timepoints, this is already approximately \(2^{25}\) voxels with just a single channel.
I.e. fMRI we expect to be two to three orders of magnitudes more costly to process than 2D images.

Similar rough calculations show that the leap from colour 2D to colour video would be only a one to
two order of magnitude increase, and yet still much of the focus of existing implemeentations for 2D
video processing focus on performance challenges
\citep[e.g.][]{tranCloserLookSpatiotemporal2018,tranVideoClassificationChannelSeparated2019,wangVideoModelingCorrelation2020}.

fMRI is truly 4D (one-channel) spatio-temporal data: temporal information is \emph{not} channel
data, as the order of the volumes is (presumably) important in fMRI. To properly process this would
require 4D convolution, which is not implemented in most popular frameworks, and would be too
computationally expensive to use practically. We thus experiment with a number of techniques to reduce
computational costs, as documented in the \hyperref[sec:methods]{Methods}.


\subsection{Existing Approaches and Benchmarks} \label{existing-attempts}

TODO: Verbally summarize other overall details of existing studies not already covered in Table
\ref{tab:existing-attempts}. E.g. summarize:

\begin{itemize}
  \item FC approaches based on ROI mean signal correlations
  \item graph-based approaches which further process the FC to extract graph metrics
  \item full approaches (no FC)
  \item embedding / autoencoder / learned representation approaches
  \item other exotic approaches (harmonization, formal modeling, ...)
\end{itemize}

\subsection{Problems with Existing Approaches}

Unfortunately, much of the research on the ABIDE dataset is of low-quality, and studies reporting
accuracies above 71\% all have non-trivial issues either in validation or the general
fitting procedure.

\subsubsection{Biased Feature Selection} \label{bias}

Suppose we have a sample of data \(\mathbf{X} \in \mathbb{R}^n\), targets \(\mathbf{y}\), a feature
selection (or engineering) procedure \(S: \mathbb{R}^n \rightarrow \mathbb{R}^m\) parameterized by
\(\omega\), and a model (e.g. classifier) \(f\) with hyperparameters \(\theta\), so that we can
write \(f(\bm{x}) = f_{\theta}(\mathbf{x})\) for \(x \in S(\mathbf{X})\).  Suppose we also have some
validation procedure which operates on the outputs of \(f\) and produces some score or loss
\(\mathcal{L}(f_{\theta}(S(\mathbf{X})), \mathbf{y}) \in \mathbb{R}\) to be minimized, and computed
via some validation procedure (e.g. mean accuracy from k-fold cross-validation). Then the loss is
\(\mathcal{L}_{\theta, \omega}(\mathbf{X}, \mathbf{y})\) or \(\mathcal{L}(\theta, \omega)\),
omitting the data. Finding an optimal set of features and hyperparameters thus requires finding
optimal or good \(\theta^{\star}\) and \(\omega^{\star}\) values.

Clearly, feature selection and hyperparameter optimization can be combined conceptually and the
whole process is identical to the usual optimization problem where we attempt to minimize
\(\mathcal{L}(\eta)\) for a function \( g_{\eta} = f_{\theta} \circ S_{\omega}\) where \( \eta =
(\theta, \omega) \). Either \(\omega\) and \(\theta\) can be optimized sequentially, by choosing
some starting \(\hat{\theta}\), computing \(\omega^{\star} = \argmin_{\omega}
\mathcal{L}(\hat{\theta}, \omega)\), and then computing \(\theta^{\star} = \argmin_{\theta}
\mathcal{L}(\theta, \omega^{\star})\) to get the optimal performance \(\mathcal{L}(\theta^{\star},
\omega^{\star})\), or in combination, by simply finding \(\argmin_{\eta} \mathcal{L}(\eta)\)\footnote{Of course the sequential optimization could be performed in the reverse order}.

If \(\mathcal{L}\) is also the final performance metric (e.g. the
misclassification error), then to prevent  biased performance (over)estimates, some kind of holdout or cross-validation
procedure is needed. That is, we must partition \((\mathbf{X}, \mathbf{y})\) into mutually disjoint sets
\((\tsub{X}{train}, \tsub{y}{train})\), \((\tsub{X}{test}, \tsub{y}{test})\), and optionally
\((\tsub{X}{val}, \tsub{y}{val})\), and ensure that  \((\tsub{X}{test}\) and \(\tsub{y}{test})\) are
\emph{never} used in any optimization procedure.

Feature selection is \emph{not} exempt from this requirement. Using all data in feature selection
means the reported result is \(\mathcal{L}_{\eta^{\star}}(\mathbf{X}, \mathbf{y})\), where
\(\eta^{\star} = \argmin_{\eta} \mathcal{L}_{\eta}(\mathbf{X}, \mathbf{y})\), which is a
\emph{biased} and \emph{overfit} estimate of performance (regardless if \(\mathcal{L}\) uses k-fold
internally) because such a procedure uses \emph{all} the data multiple times, and simply chooses the
largest value. With this procedure, there is no way to know if feature selection or engineering has
actually resulted in meaningful improvements compared to no feature selection/engineering.

In fact, as most DL is viewed as performing implicit feature engineering, choosing the "best"
feature set using all data, and then reporting results only for that feature set would be the
equivalent in deep learning of performing multiple model runs \emph{and multiple evaluations on the
test set}, and then just reporting the "best" test result. Thus, any study which performs biased
feature selection cannot be meaningfully compared to a study that makes proper use of a fully held
out test set, i.e. one which reports \(\mathcal{L}_{\eta^{\star}}(\tsub{X}{test}, \tsub{y}{test})\),
where \(\eta^{\star} = \argmin_{\eta}  \mathcal{L}_{\eta}(\tsub{X}{train}, \tsub{y}{train})\) or
\(\eta^{\star} = \argmin_{\eta} \mathcal{L}_{\eta}(\tsub{X}{val}, \tsub{y}{val}) \)

This "double-dipping" \citep{kriegeskorteCircularAnalysisSystems2009} problem is well-known and not
at all benign, and for data like MRI, can inflate accuracy estimates by tens of percentage points
\citep{wen:hal-02105134}. Unfortunately, many studies on ABIDE I reporting overall accuracies
exceeding 71\% either clearly involve such a biased feature-selection procedure, or provide
insufficient details to rule out whether feature selection was biased in this manner (see
\hyperref[tab:existing-attempts]{Table \ref{tab:existing-attempts}},  "Biased" column). We discuss
how we avoid this problem in the \hyperref[sec:methods]{Methods}.


\subsubsection{Heterogeneity and Small Validation Sets}

For heterogeneous data such as ABIDE, with 20 different data sources, validation itself poses a
unique challenge. We can see from Table \ref{tab:existing-attempts} and from Table 9 of
\citet{sakaiMachineLearningStudies2019} that with the heterogeneous ABIDE data, the apparent
validation accuracy is highest when the number of validation folds increases (or equivalently, as
the average size of the validation set decreases). This was also demonstrated on ABIDE in
\citet{iidakaRestingStateFunctional2015}, where accuracies of 77.2\%, 86.9\%, and 90.3\% for 2-, 10-,
and 50-fold respectively were obtained\footnote{These accuracies are quite high likely due to the use of an
age-restricted subset of ABIDE and biased feature selection (see section \ref{bias})}.

While this would not generally be expected from data sampled from a single
distribution, when taking small samples from a heterogeneous mixture of distributions, this outcome
is unsurprising. As the number of folds or validation sets, \(k\), approaches \(2\), the likelihood that a validation sample comes from a distribution that
was never seen even once during training increases, and as \(k\) approaches the number of samples, the reverse
occurs.

Given that ABIDE has about 20 different sites (XX from NYU and XX from ... being the majority), and
that there are at most just over 1000 subjects in the full ABIDE dataset, the reported overall
accuracy is largely a byproduct of the folding strategy and randomization or seed. Thus, assessment
of randomization failure, or approaches like leave-one-site-out (LOSO) validation or stratified
k-fold with stratification on site or scan parameters are \emph{essential} to a proper assessment of
model performance on the ABIDE dataset\footnote{although mean accuracy on e.g. LOSO will also be
inflated relative to 5- or 10-fold, as most sites have only a small number of samples, and there are
almost 20 sites)}.

Without such safeguards, one could unwittingly simply stumble upon a seed which performs well simply
due to randomization failure. Since most serious ABIDE studies have approaching 1000 samples, an
average of just 1 more correctly classified subject per fold will result in an overall accuracy
increase of 0.5\% for 5-fold, and 1\% for 10-fold. Since ABIDE contains data from 20 different
sites, it is not hard to see that one could achieve apparently impressive gains in overall accuracy,
even with k-fold, simply by "seed hacking" (perhaps quite unwittingly) until validation sets overall
tend to contain subjects from already-seen distributions \citep{picardTorchManualSeed2021}.

Of the studies reviewed in this paper, \emph{none} except
\citet{ingalhalikarFunctionalConnectivitybasedPrediction2021} and
\citet{byeonArtificialNeuralNetwork2020} mention or consider stratification or report fold site
distributions. Thus it is unclear whether strangely impressive results like a 5-fold accuracy of
over 75\% in \citet{yangDeepNeuralNetwork2020} are the result of model design, validation
procedures, or "lucky" splits.

Additionally, all the reported state-of-the-art overall accuracies are just within a few percentage
points of each other. Even on datasets orders of magnitudes larger, differences in seeds can result
accuracy differences this large \citep{picardTorchManualSeed2021}. Thus without detailed reporting
of validation splits, or setting aside established test subjects, it is almost impossible to
determine if apparent model improvements are actually meaningful. We also make efforts to improve
this state of affairs.

\section{Methods}


An fMRI image \(\bm{F}\) is a tensor or array with shape \texttt{(H,W,D,T)}, where \texttt{H},
\texttt{W}, \texttt{D}, and \texttt{T} are the sizes of the height, width, depth, and time
dimensions, respectively. The flattened array \(F\) thus has shape \((N = \texttt{H} \times
\texttt{W} \times \texttt{D}, \texttt{T})\), such that \texttt{N} is the total number of voxels in
the image (air voxels included). A region of interest (ROI) we denote with \(R\), and each of the
\(n\) ROIs \(R_i\) of \(\bm{F}\) has shape \((N_i, \texttt{T})\), where \(\sum_i^n N_i= N\). ROIs
are defined by an \emph{atlas}, which is an array of integers of shape \texttt{(H,W,D)}, with
integer values up to \(n\).

\subsection{ROI-Based Feature Extraction}

By far the vast majority of studies (see \hyperref[tab:existing-attempts]{Table
\ref{tab:existing-attempts}}) use atlas-based ROIs in order to convert the 4D fMRI image to a 2D
array. Either ROI mean signals are used, as in \citet{el-gazzarHybrid3DCNN3DCLSTM2019}, or the
(upper triangle) of the matrix of correlations between all such mean signals are used as input
features.

\subsection{}

\subsection{Feature Extraction via Correlation Eigenvalues}

DUe to the multi-site nature of the ABIDE data, the number of timepoints and repetition time (TR)
vary across scans. ([TODO]: see Table or Histogram). However, deep or machine learning algorithms
require inputs to be of the same size (if not generally, then in mini-batches of more than one
input). While cropping, padding or interpolation are usually used to overcome such issues, these
techniques are more difficult to justify when the underlying sampling rates differ and signals are
noisy, and are likely to lead to poor generalization and memorization when sampling rates and image
sizes vary significantly. I.e. padded inputs occupy completely different regions of space and so
will form a clearly separable cluster, cropped inputs with different sampling rates represent
different amounts of measurement time, and interpolated inputs are likely to have increased
correlations between features (voxels) unless the interpolation is downsampling and does not apply
any averaging or smoothing (e.g. as in nearest-neighbour interpolation).

Thus there is a challenge of converting all images to some common representation which is relatively
insensitive to these temporal differences, or which eliminates these differences while retaining
rich information about the original signals.

One recent finding has been that the functional connectivity of rs-fMRI is surprisingly robust to the
the underlying sampling rate \citep{huotariSamplingRateEffects2019,shakilEvaluationSlidingWindow2016}

\subsection{Previous Approaches}

TODO: Summarize various studies listed in notes below (\ref{key-articles} and \ref{other-articles})
in table or other condensed form.

% \citet{eslamiASDDiagNetHybridLearning2019} use an autoencoder to


\section{Methods} \label{sec:methods}

\subsection{Data}

\subsubsection{ABIDE Data}

Publicly availabile fMRI data is limited. While databases such as OpenNeuro
\citep{markiewiczOpenNeuroOpenResource2021} exist and cover a wide variety of domains, the number
of subjects is typically far too small for machine learning applications, and the studies are far
too heterogenous to be usefully combined. Perhaps the most popular publicly available fMRI dataset
is the ABIDE I dataset, which includes 1112 resting-state fMRI (rs-fMRI) scans from 539 subjects
with autism spectrum disorders (ASD) and 573 age-matched controls with typical development (TD)
\citep{dimartinoAutismBrainImaging2014}.

The ABIDE I dataset is an appropriate size for ML and DL approaches, but is particularly challenging
due to the inclusion of subjects from 20 different sites. This results in highly varied demographic
and psychological characteristics and scanning parameters. While this heterogeneity makes ML and DL
approaches difficult, it also means that if such an approach performs well on this data, that it has
much more potential clinical utility and relevance.


The ABIDE I dataset \citep{dimartinoAutismBrainImaging2014} is a publicly acessible rs-fMRI dataset of over
1112 subjects, 539 of which are diagnosed with Autism Spectrum Disorder (ASD),
and 573 of which are typical development (TD). The data are collected from 17 different sites with
varying scan parameters, and subjects vary considerably in age, making the dataset analytically
challenging. However, the high heterogeneity of the data also makes it more suitable for testing the
generalizability of modern predictive methods, and methods that perform well on the ABIDE data
\emph{a priori} have more potential clinical utility.

The ABIDE I data is available fully-preprocessed, and investigators can choose from a number of
pre-processing pipelines and options. The main options involve filtering (bandpass filtering, global
signal regression), and a choice of one of four pipelines \citep{dimartinoAutismBrainImaging2014}.
However, to keep analyses consistent with similar papers
\citet{abrahamDerivingReproducibleBiomarkers2017, mostafaDiagnosisAutismSpectrum2019,
yinDiagnosisAutismSpectrum2021, heinsfeldIdentificationAutismSpectrum2018} we use the subjects from
the Configurable Pipeline for the Analysis of Connectomes
\citep[CPAC;][]{cameronAutomatedAnalysisConnectomes2013}, and exclude subjects that fail to pass quality
control checks from three independent experts \citep[see][for
details]{abrahamDerivingReproducibleBiomarkers2017}, giving a final total of 871 subjects (XXX ASD,
XXX TD).

\subsubsection{\href{http://preprocessed-connectomes-project.org/adhd200/}{ADHD-200}}

TODO: If things all work well with ABIDE, download this dataset and use our methods, and also
compare to existing research there.



\subsection{Feature Extraction}
\subsubsection{ROI Means}

These are available from the CPAC pipeline for direct download [cite]. In the interest of reproducibility we simply
use these directly.

\subsubsection{ROI Standard Deviations}

Each ROI







\newpage
\begin{landscape}

\begin{table}
  \small
  \centering
  \begin{tabular}{lcccccccccc}
    \toprule
    Study                                                         & Pipeline & Model  & Features & Biased &Atlas or \(N_{ROI}\)  & \(N\) & \(N_\text{ASD}\) & \(N_{\text{TD}}\) & Validation & OA \\
    \midrule
    \citeauthor{el-gazzarHybrid3DCNN3DCLSTM2019}                       &   CPAC   & Conv1D & ROI means&   N    &     HO     & 1100  &   ?   &   ?   &   5-fold   &       64.0       \\
    \citeauthor{yangDeepNeuralNetwork2020}                             &   CPAC   &  ANN   &  ROI-FC  &   N    &   CC-400   & 1035  &  505  &  530  &   5-fold   &     \(75.27^a\)  \\
    \citeauthor{almuqhimASDSAENetSparseAutoencoder2021}                &   CPAC   & AE+ANN &  ROI-FC  &   N    &   CC-200   & 1035  &  505  &  530  &  10-fold   &       70.8       \\
    \citeauthor{eslamiASDDiagNetHybridLearning2019}                    &   CPAC   & AE+ANN &  ROI-FC  &   N    &   CC-200   & 1035  &  505  &  530  &  10-fold   &       70.3       \\
    \citeauthor{sherkatghanadAutomatedDetectionAutism2020}             &   CPAC   &  CNN   &  ROI-FC  &   N    &   CC-400   & 1035  &  505  &  530  &  10-fold   &       70.2       \\
    \citeauthor{heinsfeldIdentificationAutismSpectrum2018}             &   CPAC   &   ANN  &  ROI-FC  &   N    &   CC-200   & 1035  &  505  &  530  &  10-fold   &       70.0       \\
    \citeauthor{ingalhalikarFunctionalConnectivitybasedPrediction2021} &  DPARSF  &  ANN   &  ROI-FC+ &   ?    &   CC-200   &  988  &  432  &  556  &10-fold LOSO&       71.4       \\
    \citeauthor{wangMAGEAutomaticDiagnosis2021}                        &   CPAC   &GCN+ensemble&ROI-FC&   ?    &  multiple  &  949  &  419  &  530  &  10-fold   & 75.9 (70.7-72.5)\(^b\)  \\
    \citeauthor{yinDiagnosisAutismSpectrum2021}                        &  custom  & AE+SVM &  graph   &   N    & \(264^c\)  & \(871^d\) & \(403^d\) & \(468^d\)  &  holdout\(^d\) & \(78.3^d\) \\
    \citeauthor{shaoClassificationASDBased2021}                        &   CPAC   & FS+GCN &  ROI-FC  &   Y    &     HO     &  871  &  403  &  468  &  10-fold   &       79.5       \\
    \citeauthor{mostafaDiagnosisAutismSpectrum2019}                    &  custom  & FS+LDA &  graph   &   ?    & \(264^c\)  &  871  &  403  &  468  &  10-fold   &       77.7       \\
    \citeauthor{parisotDiseasePredictionUsing2018}                     &   CPAC   & FS+GCN &  graph   &   ?    &     HO     &  871  &  403  &  468  &  10-fold   &       70.4       \\
    \citeauthor{khoslaEnsembleLearning3D2019}                          &   CPAC   &  CNN   &  ROI-FC+ &   N    &   CC-200  &\(774/393^e\)&\(379/163^e\)&\(395/230^e\)&holdout& 72.8 \\
    \citeauthor{iidakaRestingStateFunctional2015}                      &  custom  & FS+PNN &  ROI-FC  &   Y    &     AAL   &\(640^f\)& 312  &  328  &  10-fold   &       86.9       \\
    \citeauthor{byeonArtificialNeuralNetwork2020}                      &   CPAC  & CNN+RNN+ & graph   &   N    &     BNA    &\(575^g\)& 270 &  305  & 5-fold\(^h\)&       74.5      \\
    \citeauthor{wangFunctionalConnectivitybasedClassification2019}     &  DPARSF  & FS+SVM &  ROI-FC  &   Y    &     35     &  531  &  255  &  276  &   LOSO    &      75.0-95.2    \\
    \citeauthor{liMultisiteFMRIAnalysis2020}                           &   CPAC   &  ANN   &  ROI-FC  &   N    &     HO     &  370  &  186  &  184  &   5-fold   & 67.6\% to 84.9\% \\
    \citeauthor{kazeminejadTopologicalPropertiesRestingState2019}      &   CPAC   & FS+SVM &  graph   &   N    &     AAL   &\(342^i\)&  ?   &   ?   &  10-fold   &       69.0       \\
   %\citeauthor{dekhilUsingRestingState2018}                           &          &        &          &        &            &       &       &       &            &                  \\
    \bottomrule
  \end{tabular} \label{tab:existing-attempts}
  \caption{
    Best overall accuracies of existing approaches using only over 200 subjects and rigorous validation strategies. Sorted by sample size and then accuracy.
    \(N\) = total number of subjects,
    \(N_{\text{ASD}}\) = total number of ASD subjects,
    \(N_{\text{TD}}\) = total number of TD subjects.
    DPARSF = \citep[see][]{yanDPARSFMATLABToolbox2010}.
    LOSO = Leave-one-site-out cross-validation.
    LOOCV = leave-one-out cross-validation.
    ANN = Aritificial Neural Network (linear layers).
    PNN = Probabilistic neural network \citep{spechtProbabilisticNeuralNetworks1990}.
    CNN = Convolutional neural network
    RNN = Recurrent neural network
    GCN = graph convolutional network.
    AE = Autoencoder. RF = Random forest. SVM = support vector machine.
    LDA = Linear Discriminant Analysis.
    ROI-FC = ROI-based FC matrix ("+" indicates extra processing on ROI-FC).
    graph = graph-theoretic measures and metrics, usually extracted from ROI-FC.
    DA = domain adaptation
    CCS = Connectome Computation System \citep{xuConnectomeComputationSystem2015}.
    CC-200/400 = Cameron-Craddock 200/400.
    HO = Harvard-Oxford Atlas.
    AAL = Automated anatomical labeling \citep{tzourio-mazoyerAutomatedAnatomicalLabeling2002}.
    BNA = BrainNetome Atlas \citep{fanHumanBrainnetomeAtlas2016}
    OA = Overall Accuracy.
    FS = feature selection.
    Biased = feature selection using full data.
    \\\hspace{\textheight} \(^a\) Note this paper also reports 5-fold accuracies of 71-72\% with simple
    logistic regression, ridge regression, and linear SVM, which seems very difficult to square with
    other studies reviewed here, especially since 5-fold should be much more difficult than 10-fold,
    unless perhaps stratification is involved
    \\\hspace{\textheight} \(^b\) Bracketed values are values without ensembling.
    \\\hspace{\textheight} \(^c\) 264 ROI Atlas from \citep{powerFunctionalNetworkOrganization2011}
    \\\hspace{\textheight} \(^d\) Authors in fact use 80\% of data for training and feature selection,
    and validate using 10-fold only on a final set held-out from feature selection of 174 subjects.
    \\\hspace{\textheight} \(^e\) Train/test. Training subjects from ABIDE I, testing from ABIDE II.
    \\\hspace{\textheight} \(^f\) Only subjects less than 20 years old were used for this study.
    \\\hspace{\textheight} \(^g\) Included only subjects with a TR of 2.0s
    \\\hspace{\textheight} \(^h\) Stratified for both ASD/TD and site ratios
    \\\hspace{\textheight} \(^i\) Only subjects between 10-15 years of age.
  }
  \normalsize
\end{table}
\end{landscape}
\section{Key Articles} \label{key-articles}

Note for below I also seem to use the CPAC minimal pipeline, I think.

Of studies that get above e.g. 70.3\% accuracy, there is \emph{almost always} some cheaty feature
selection using all subjects, completely negating the final 10-fold or whatever accuracy values. No
\emph{honest} techniques make it above 70\% accuracy across sites yet \emph{except}
\citet{mostafaDiagnosisAutismSpectrum2019,yinDiagnosisAutismSpectrum2021}, who use eigenvalues of
the graph of the Laplacian of the thresholded correlation matrix.

\subsection{Use of Eigenvalues for Prediction! \citet{mostafaDiagnosisAutismSpectrum2019}} \label{eig-pred}



\subsection{\citet{heinsfeldIdentificationAutismSpectrum2018}}

\begin{itemize}
  \item 70\% with C-PAC preprocessing pipeline
  \item used CC200 atlas to reduce to ROI feature vectors (i.e. not CNN)
  \item actually used functional connectivity as the predictor
  \item used 10-fold validation across all sites (good!)
  \item used a deep denoising autoencoder (really just MLP with some augmentation / dropout on inputs)
\end{itemize}
\subsection{\citet{eslamiASDDiagNetHybridLearning2019}}

Decent. Very similar to ours in spirit.

\begin{itemize}
  \item 1112 subjects from almost all sites
  \item CPAC pipeline with CC200 functional ROIs
  \item use only some correlations (e.g. select smallest and largest correlations (most negative?))
  \item train single-layer autoencoder first to do e.g. a non-linear PCA first for reduction /
  embedding
  \item use SMOTE to \textbf{augment their data!} (basically, linearly interpolate a new sample from
  5 nearest samples), effectively doubles the training set size
  \item get to 70.3\% accuracy (their augmentation adds only 1\%)
\end{itemize}

\subsection{\citet{yinDiagnosisAutismSpectrum2021}}

\textbf{VERY GOOD}, same authors as \citet{mostafaDiagnosisAutismSpectrum2019}.

\begin{itemize}
  \item 871 subjects, custom preprocessing (minimal), 264 ROI parcellation, Pearson corrs
  \item also threshold map to largest (in abs. value) correlations (e.g. all correlations greater in
  absolute value then a threshold go to 1 or -1, so matrix becomes an adjacency matrix)
  \item then compute a Laplacian matrix (very simple, see article) from adjacency matrix
  \item then get eigenvalues from this
  \item compute a bunch of other graph-theoretic metrics from this Laplacian matrix
  \item interestingly \textbf{min-max normalize} their eigenvalues within subjects
  \item do not contaminate their autoencoder (proper splitting of training and testing)
  \begin{itemize}
    \item however the splitting is a little bit fucky since it means pre-training
  \end{itemize}
  \item without autoencoder pre-training and just using their graph features, get DNN acc of 76.2\%
  \begin{itemize}
    \item interestingly find that need a lower threshold (allow all correlations above 0.2) for this
    deep learner, which might be relevant for me since right now I am taking only top 25
    eigenvalues, which is probably not enough (see Table 2 of their study, very interesting!)
  \end{itemize}
\end{itemize}

\subsection{\citet{khoslaEnsembleLearning3D2019}}
\textbf{Note that choice of atlas doesn't matter}, we could use this as a citation for motivating
our non-atlas-based approach! Also this is a clever / good study that combines a lot of tricks and does seem to manage to get to 72.8\%.
\begin{itemize}
  \item CPAC pipeline, but did some extra expert / manual scrubbing and quality control (manual and
  automatic), exclude very young and very old, which brought them down to  \textbf{163 ASD / 230
  CTRL}
  \item looked at all ROIs from all atlases
  \item get to about 72.8\% acc with CC200 parcellation and 3D-CNN (Table 2)
  \item various other models all get to around 71.2\%, 71.7\%, 72.3\%, etc
  \item also generate a novel / clever new multi-channel 3D image where each channel is some different
  feature extracted from the connectivity info (e.g. channel 1 is voxel time series correlation with mean global time series, channel 2 is some other..., etc)
\end{itemize}

\section{Other Articles} \label{other-articles}


\subsection{\citet{liMultisiteFMRIAnalysis2020}}

\begin{itemize}
  \item 70\% with C-PAC preprocessing pipeline
  \item similar number of subjects to us
  \item used sliding windows (32 timepoints long) of mean ROI sequences (HO Atlas, 111 ROIs)
  \item interesting comparison points is \emph{sex classification accuracy} which was only between
  \item report no overall accuracy, but NYU hardest to classify (67.6\% at best), USM easiest (up to
  84.9\%)
  \item validation splits are unclear
\end{itemize}

\subsection{\citet{el-gazzarHybrid3DCNN3DCLSTM2019}}

\begin{itemize}
  \item used Conv3D to Conv1D or ConvLSTM (they test both)
  \item CPAC pipeline for ABIDE
  \item selected for and cropped to 100 timepoints for the ABIDE-I dataset
  \item only use single-site validation (losers) also NYU and UM (easiest)
  \item also do one multi-site with 19 sites 1100 subjects
  \item patch-based training where prediction is average prediction over crops
  \item max 5-fold acc of \(0.77 \pm 0.05\) on NYU with Conv3dConvLSTM3d
\end{itemize}

\begin{table}
\caption{\citet{el-gazzarHybrid3DCNN3DCLSTM2019} 5-fold cross-site results}
\centering
  \begin{tabular}{lll}
    \toprule
    % \multicolumn{2}{c}{Part}                   \\
    % \cmidrule(r){1-2}
    Model     & Accuracy     & F1-score \\
    \midrule
    AE MLP        [8]   &  0.63 \(\pm\) 0.02  & 0.64 \\
    SVM           [5]   &  0.58 \(\pm\) 0.04  & 0.60 \\
    1D Conv       [7]   &  0.64 \(\pm\) 0.06  & 0.64 \\
    3DCNN 1D     (ours) &  0.54 \(\pm\) 0.02  & 0.50 \\
    3DCNN C-LSTM (ours) &  0.58 \(\pm\) 0.03  & 0.53 \\

    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{\citet{shaoClassificationASDBased2021}}

\begin{itemize}
  \item GCN (graph convolutional network) plus another network that learns feature weights
  \item use CPAC preprocessed data, but \textbf{discard 241 ghosty / bad-looking data} to get a
  total of 871 subjects
  \item used mean series on some of HO (Harvard-Oxford) atlas, standardized to zero mean and unit variance
  \item 111 ROIs total
  \item do feature selection and also test various models
  \item GCN gets \(79.5\pm3.3\)\% acc (3.3 is sd), 10-fold validation
\end{itemize}

\subsection{\citet{dekhilUsingRestingState2018}}
\begin{itemize}
  \item  fMRI data for 123 ASD and 160 TD children and adolescents (for a total number of 283
  subjects) from the National Database for Autism Research (NDAR: http://ndar.nih.gov)
  \item all Ps have both MRI and rs-fMRI scans
  \item do register, BET, slicetime, motion, and then spatial Gaussian spatial smoothing
  \item extract 34 ROIs via ICA
  \item features for prediction are the \textbf{power spectral densities}
  \item get very high accuracy, but isn't ABIDE, and is one site
\end{itemize}

\subsection{\citet{parisotDiseasePredictionUsing2018}}
\begin{itemize}
  \item ``Our analysis shows that our novel framework can improve over state-of-the-art results on
  both databases, with 70.4\% classification accuracy for ABIDE''
  \item 871 subjects (remove a lot of bad scans) from CPAC pipeline
  \item GCN on correlation matrices between HO atlas mean ROI signals (plus some fancy normalization
  / filtering), with recursive feature selection
  \item stratified grouped 10-fold to get the 70.4\% acc
\end{itemize}

\subsection{\citet{sakaiMachineLearningStudies2019}}

See Table 9 of this one for ABIDE accuracies. There are some spuriously high and extremely unlikely
out-of-bag accuracies of 0.90 or on tiny sample. Otherwise, of note is
\citet{iidakaRestingStateFunctional2015} who apparently get an LOOCV of 0.90 (??? dubious).

\subsection{\citet{iidakaRestingStateFunctional2015}}

\begin{itemize}
  \item only subjects under 20
  \item 640 subjects total
  \item 2-fold acc was 77.2\%, 10-fold was 86.9\%
  \item did do some bandpass filtering as pre-processing, drop first 5 volumes
  \item features are 90 AAL mean normalized ROI regions correlation matrix (r-values fisher
  normalized to Z-scores)
\end{itemize}

\subsection{\citet{liNovelTransferLearning2018}}

\begin{itemize}
  \item pre-train stacked autoencoder on different data healthy rs-fMRI first
  \item ABIDE used Connectome Computation System preprocessed
  \item \textbf{regressed out height, age, sex, site} from among ROI correlations
  \item never get above 70.4\% acc, and only within USM, so not good
\end{itemize}

\subsection{\citet{kazeminejadTopologicalPropertiesRestingState2019}}

\begin{itemize}
  \item CPAC pipeline, 116 AAL regions
  \item features are multiple correlation matrices (e.g. Pearson, Spearman, partial correlation,
  mutual information) converted to graphs via thresholding
  \item Gaussian SVM classifier (sklearn), step-up feature selection
  \item validation is 10-fold, but very dubious procedure, sounds / unclear how feature selection is
  being done here, almost certainly overfitting a lot
  \item no overall reported accuracy, just report stratified by ages, sad, lame, (lol Frontiers)
\end{itemize}


\subsection{\citet{khoslaEnsembleLearning3D2019}}

\textbf{Note that choice of atlas doesn't matter}, we could use this as a citation for motivating
our non-atlas-based approach! Also this is a clever / good study that combines a lot of tricks and does seem to manage to get to 72.8\%.

\begin{itemize}
  \item CPAC pipeline, but did some extra expert / manual scrubbing and quality control (manual and
  automatic), exclude very young and very old, which brought them down to  \textbf{163 ASD / 230
  CTRL}
  \item looked at all ROIs from all atlases
  \item get to about 72.8\% acc with CC200 parcellation and 3D-CNN (Table 2)
  \item various other models all get to around 71.2\%, 71.7\%, 72.3\%, etc
  \item also generate a novel / clever new multi-channel 3D image where each channel is some different
  feature extracted from the connectivity info (e.g. channel 1 is voxel time series correlation with mean global time series, channel 2 is some other..., etc)
\end{itemize}

\subsection{\citet{wangIdentifyingAutismSpectrum2020}}

\begin{itemize}
  \item 468 subjects, CPAC pipeline, AAL 116 mean time series
  \item don't really report clean overall accuracies because they are doing domain adaptation
  (training on non-NYU data and predicting NYU data) but basically when having similar validation
  sizes as us (e.g. about 40\%) are also in the 69-73\% range of accuracy
\end{itemize}

\subsection{\citet{sherkatghanadAutomatedDetectionAutism2020}}

\textbf{Would be good to try to replicate these dubious results}

\begin{itemize}
  \item CPAC pipeline, 871 "quality" images, CC400 parcellation
  \item shallow (but wide) CNN used directly on correlation matrix
  \item 10-fold acc of 70.2\%
\end{itemize}

\subsection{\citet{wangFunctionalConnectivitybasedClassification2019}}

Very curious, given how shallow network is and how simple approach is...

\begin{itemize}
  \item reports a crazy cross-site accuracy of \textbf{over 90\%}
  \item  531 subjects(255AD / 276CTRL), DPARSF pipeline, also did some QC
  \item 75-95\% acc based on "leave-one-site-out"
  \item 90.6\% acc on all sites with SVM
  \item key thing is a recursive, SVM-based step-down feature selection procedure on ROIs
  \item \textbf{HOWEVER is overfit because all 531 subjects used in feature selection... lol}
\end{itemize}

\subsection{\citet{yangDeepNeuralNetwork2020}}

Probably \textbf{worth trying to replicate this} given the stupid simplicity

\begin{itemize}
  \item CC400 atlas, ROI correlation matrix, pathetically simply MLP, 1035 subjects
  \item claim to get 5-fold acc of 75.3\%
  \item cross-validated grid-search of classifiers, so perhaps just overfitting the extra 4\%?
\end{itemize}

\subsection{\citet{wangMAGEAutomaticDiagnosis2021}}

\begin{itemize}
  \item 949 subjects, CPAC pipeline
  \item 75.86\% accuracy, 10-fold
  \item multi-atlas Graph CNNs (ensemble learning), recursive feature elimination
  \item show single-atlas based models get 70-72\% acc
  \item \textbf{feature selection subjects completely unclear, another cheat}
\end{itemize}

\subsection{\citet{ingalhalikarFunctionalConnectivitybasedPrediction2021}}

\begin{itemize}
  \item most subjects, DPARSF pipeline, "cross-site harmomnization" with denoising autoencoder
  \item 71.35\% 10-fold ACC with ANN
\end{itemize}

\subsection{\citet{yangLargeScaleBrainFunctional2021}}

77.74\% 1-fold mean acc but with small number of subjects, only NYU (lame)

\subsection{\citet{almuqhimASDSAENetSparseAutoencoder2021}}

\begin{itemize}
  \item claims \citet{eslamiASDDiagNetHybridLearning2019} is current state of art (70.3\%)
  \item CPAC pipeline, CC200, correlation matrix, only one triangle, 1/4 largest, 1/4 smallest
  \item sparse autoencoder to force embedding
  \item get 10-fold overall acc of 70.8\%
\end{itemize}

\subsection{\citet{byeonArtificialNeuralNetwork2020}}

\begin{itemize}
  \item CPAC, only subjects with TR=2.0 (270 ASD, 305 TD) and good QC
  \item BrainNetome atlas mean signals (246 ROI)
  \item use first 146 timepoints only to make subjects all same size
  \item 74.5\% accuracy, 5-fold
\end{itemize}

\section{Headings: first level}
\label{sec:headings}

See Section \ref{sec:headings}.

\subsection{Headings: second level}
\paragraph{Paragraph}

A footnote \footnote{Sample of the first footnote.}.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}