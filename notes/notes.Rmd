---
title: "ABIDE NotesTitle"
fontsize: 10pt
urlcolor: blue
linkcolor: blue
mainfont: "Garamond Premier Pro"
biblio-style: "apalike"
# bibliography: "biblography.bib"  # exported from Zotero with better BibLaTeX extension
link-citations: true
output:
    # bookdown::pdf_document2:
    bookdown::pdf_book:
        latex_engine: xelatex
        toc: true
        # toc_depth: 2
        number_sections: false
        highlight: tango
        # https://bookdown.org/yihui/rmarkdown-cookbook/figure-placement.html
        # unicode-math needed for bold italic math
        # (https://tex.stackexchange.com/questions/431013/error-extended-mathchar-used-as-mathchar-when-using-bm)
        extra_dependencies: ["float", "unicode-math"]
        mathjax: default
        includes:
            in_header: preamble.tex
        split_bib: true
        fig_width: 6
        fig_height: 4
        link-citations: true
citation-style: apa.csl # https://www.zotero.org/styles
---


\newpage


# Data Exploration

There are 1112 subjects, 523 controls, 539 autism. Controls are 474 male, autism is 474 male.

```
>>> pd.concat([ctrl.describe().loc[:, "AGE_AT_SCAN"], autism.describe().loc[:, "AGE_AT_SCAN"]], axis=1)
       AGE_AT_SCAN  AGE_AT_SCAN
count   573.000000   539.000000
mean     17.082463    17.013146
std       7.720320     8.366424
min       6.470000     7.000000
25%      11.800000    11.530000
50%      14.800000    14.420000
75%      20.650000    19.672150
max      56.200000    64.000000
```

Full IQs span a pretty broad range:

```
>>> col = "FIQ"; pd.concat([ctrl.describe().loc[:, col], autism.describe().loc[:, col]], axis=1)
             CTRL      AUTISM
count  535.000000  505.000000
mean   111.245981  105.345743
std     12.333965   17.005317
min     73.000000   41.000000
25%    103.500000   94.000000
50%    111.000000  105.000000
75%    120.000000  118.000000
max    148.000000  148.000000
`
```


## A Simple Linear MLP


As a concrete example, imagine we are build a simple ANN from $\mathbbm{R}$ to $\mathbbm{R}^2$ by
way of two larger intermediate layers. We might have for example

```python
ann = Sequential([
    Linear(1, 2),
    Linear(2, 1),
])
```

which learns the weights

$$
\begin{aligned}
\mathbf{W}_1 &= \begin{bmatrix} w_1 & w_2 \end{bmatrix} & \text{layer 1: matrix of shape (1, 2), output shape (1, 2)} \\
\mathbf{W}_2 &= \begin{bmatrix} w_3 \\ w_4 \end{bmatrix}& \text{layer 2: matrix of shape (2, 1), output shape (1, 1)} \\
\end{aligned}
$$

Assuming ReLUs and no biases, the entire equation for this network is
$y = \mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 x) + b$ (final layer should always have a bias), that is:

$$
\begin{aligned}
\mathbf{W}_1 x & = \begin{bmatrix} w_1 \cdot x & \;\; w_2 \cdot x \end{bmatrix} \\
             y & = \begin{bmatrix}  w_3 \cdot \text{ReLU}(w_1 \cdot x) + w_4 \cdot \text{ReLU}(w_2 \cdot x) + b \end{bmatrix} \\
\end{aligned}
$$

This in fact reduces to:

$$
y = \begin{cases}
w_1 w_3 x + w_2 w_4 x + b  & \text{ if }  sgn(x) =   sgn(w_1) \text{ and } sgn(x)   = sgn(w_2) \\
w_1 w_3 x + b              & \text{ if }  sgn(x) =   sgn(w_1) \text{ and } sgn(x) \ne sgn(w_2) \\
w_2 w_4 x + b              & \text{ if }  sgn(x) \ne sgn(w_1) \text{ and } sgn(x)   = sgn(w_2) \\
b                          & x = 0 \\
\end{cases}
$$

In particular, without loss of generality we can take $w_1 < 0$ and $w_2 > 0$ and reduce this to:

$$
y = \begin{cases}
w_2 w_4 x + b  & \text{ if }  x \ge 0 \\
w_1 w_3 x + b  & \text{ if }  x  < 0 \\
\end{cases}
$$

# References
